{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UXAjutbMhoa"
      },
      "source": [
        "#  Que peuvent faire les *transformers* ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3GIqRT-Mhoe"
      },
      "source": [
        "Installez la biblioth√®que ü§ó *Transformers* pour ex√©cuter ce *notebook*."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'outil le plus basique de la biblioth√®que ü§ó *Transformers* est la fonction `pipeline()`. Elle relie un mod√®le avec ses √©tapes de pr√©-traitement et de post-traitement, permettant d'entrer n'importe quel texte et d'obtenir une r√©ponse compr√©hensible :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KGUwAC1zMhoj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: transformers[torch] in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (0.12.1)\n",
            "Requirement already satisfied: requests in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (2.28.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (1.24.2)\n",
            "Requirement already satisfied: filelock in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (3.9.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from requests->transformers[torch]) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from requests->transformers[torch]) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from requests->transformers[torch]) (3.1.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the '/Users/ashnaahmad/Documents/GitHub/course/.env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \"transformers[torch]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ehB754vsO78P"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\"\n",
        ")  # J'ai attendu un cours d'HuggingFace toute ma vie."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "classifier = pipeline(\"application\")\n",
        "classifier(\"sentence\", \"sentence\") to run multiple sentences through the application"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Par d√©faut, ce pipeline s√©lectionne un mod√®le pr√©-entra√Æn√© qui a √©t√© sp√©cifiquement entra√Æn√© pour l'analyse de sentiment en anglais. Le mod√®le est t√©l√©charg√© et mis en cache lorsque vous cr√©ez l'objet `classifier`. Si vous r√©ex√©cutez la commande, c'est le mod√®le mis en cache qui sera utilis√© et il n'y a pas besoin de t√©l√©charger le mod√®le √† nouveau.\n",
        "\n",
        "Il y a trois √©tapes principales lorsque vous passez du texte √† un pipeline :\n",
        "\n",
        "1. le texte est pr√©trait√© pour qu'il ait un format compr√©hensible par le mod√®le,\n",
        "2. les donn√©es pr√©trait√©es sont pass√©es au mod√®le,\n",
        "3. les pr√©dictions du mod√®le sont post-trait√©es de sorte que vous puissiez les comprendre.\n",
        "\n",
        "\n",
        "Voici une liste non-exhaustive des [pipelines disponibles](https://huggingface.co/transformers/main_classes/pipelines.html) :\n",
        "\n",
        "- `feature-extraction` (pour obtenir la repr√©sentation vectorielle d'un texte)\n",
        "- `fill-mask`\n",
        "- `ner` (*named entity recognition* ou reconnaissance d'entit√©s nomm√©es en fran√ßais)\n",
        "- `question-answering`\n",
        "- `sentiment-analysis`\n",
        "- `summarization`\n",
        "- `text-generation`\n",
        "- `translation`\n",
        "- `zero-shot-classification`\n",
        "\n",
        "Regardons de plus pr√®s certains d'entre eux !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VmlNOR-OdIU"
      },
      "source": [
        "### Analyse de sentiments"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "i guess a particular model as well as a particular application is also a parameter of pipeline (see below). also when using a specific model, make sure it uses the right framework (pt/tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ILYepD-3Mhom"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uYmC3B-ZMhoo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 896/896 [00:00<00:00, 90.8kB/s]\n",
            "Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443M/443M [00:34<00:00, 13.0MB/s] \n",
            "Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 631/631 [00:00<00:00, 52.3kB/s]\n",
            "Downloading (‚Ä¶)ncepiece.bpe.model\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 811k/811k [00:00<00:00, 1.91MB/s]\n",
            "Downloading (‚Ä¶)/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.40M/1.40M [00:02<00:00, 468kB/s]\n",
            "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [00:00<00:00, 51.5kB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.8035174608230591}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=\"philschmid/pt-tblard-tf-allocine\")\n",
        "classifier(\"J'ai attendu un cours d'HuggingFace toute ma vie.\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSiD3eYEMhos"
      },
      "source": [
        "Int√©ressant ! On observe que le r√©sultat est n√©gatif l√† o√π pour la version en anglais le r√©sultat est positif."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jo4BXTXMhou"
      },
      "outputs": [],
      "source": [
        "classifier(\n",
        "    [\"J'ai attendu un cours d'HuggingFace toute ma vie.\", \n",
        "     \"Je d√©teste tellement √ßa !\"]\n",
        ") # pour classifier plusieurs phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpC8hlc_OBc8"
      },
      "source": [
        "La phrase \"J'ai attendu un cours d'HuggingFace toute ma vie.\" qui √©tait pr√©cedemment n√©gative devient √† pr√©sent positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgmwK2d6OlFE"
      },
      "source": [
        "### Z√©ro shot classification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous allons commencer par nous attaquer √† une t√¢che plus difficile o√π nous devons classer des textes qui n'ont pas √©t√© annot√©s. C'est un sc√©nario tr√®s r√©pandu dans les projets r√©els car l'annotation de textes est g√©n√©ralement longue et n√©cessite parfois une expertise dans un domaine. Pour ce cas d'usage, le pipeline `zero-shot-classification` est tr√®s puissant : il vous permet de **sp√©cifier les labels** √† utiliser pour la classification, de sorte que **vous n'ayez pas √† vous soucier des labels du mod√®le pr√©-entra√Æn√©**. Nous avons d√©j√† vu comment le mod√®le peut classer un texte comme positif ou n√©gatif en utilisant ces deux labels mais **il peut √©galement classer le texte en utilisant n'importe quel autre ensemble de labels que vous souhaitez.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "agGiUvz5Mho1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 882/882 [00:00<00:00, 86.2kB/s]\n",
            "Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443M/443M [00:07<00:00, 60.7MB/s] \n",
            "Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 433/433 [00:00<00:00, 88.5kB/s]\n",
            "Downloading (‚Ä¶)ncepiece.bpe.model\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 811k/811k [00:00<00:00, 8.59MB/s]\n",
            "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:00<00:00, 196kB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'sequence': \"C'est un cours sur la biblioth√®que Transformers\",\n",
              " 'labels': ['√©ducation', 'affaires', 'politique'],\n",
              " 'scores': [0.8710299134254456, 0.09802455455064774, 0.030945533886551857]}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(\"zero-shot-classification\", model=\"BaptisteDoyen/camembert-base-xnli\") # set model\n",
        "classifier(\n",
        "    \"C'est un cours sur la biblioth√®que Transformers\",\n",
        "    candidate_labels=[\"√©ducation\", \"politique\", \"affaires\"], # set custom models for classification, which will be assigned based on training (learned association of words with labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kujBeTwDO9_3"
      },
      "source": [
        "### G√©n√©ration de texte"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maintenant, nous allons voir comment utiliser un pipeline pour g√©n√©rer du texte. L'id√©e principale ici est que vous fournissez seulement un extrait de texte qui **va √™tre compl√©t√© par du texte g√©n√©r√© automatiquement** par le mod√®le. Cette fonction est similaire √† la fonction de texte pr√©dictif que l'on trouve sur de nombreux t√©l√©phones portables. La g√©n√©ration de texte implique de l'al√©atoire, donc il est normal que vous n'obteniez pas les m√™mes r√©sultats que ceux pr√©sent√©s ci-dessous."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vous pouvez am√©liorer votre recherche de mod√®le (model=) en cliquant sur les *filtres* de langue et choisir un mod√®le qui g√©n√®re du texte dans une autre langue. Le *Hub* contient √©galement des *checkpoints* pour des mod√®les multilingues qui supportent plusieurs langues.\n",
        "\n",
        "Une fois que vous avez choisi un mod√®le, vous verrez que vous pouvez tester son fonctionnement en ligne directement. Cela vous permet de tester rapidement les capacit√©s du mod√®le avant de le t√©l√©charger.\n",
        "\n",
        "Tous les mod√®les peuvent √™tre test√© directement depuis votre navigateur en utilisant l'API d'inf√©rence qui est disponible sur le site [Hugging Face](https://huggingface.co/). Vous pouvez jouer avec le mod√®le directement sur sa page en entrant du texte personnalis√© et en regardant le mod√®le traiter les donn√©es d'entr√©e."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Sc0NpBMho6"
      },
      "outputs": [],
      "source": [
        "generator = pipeline(\"text-generation\", model=\"asi/gpt-fr-cased-small\")\n",
        "generator(\"# Dans ce cours, nous vous enseignerons comment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb6QpQF6Mho8"
      },
      "outputs": [],
      "source": [
        "generator = pipeline(\"text-generation\", model=\"asi/gpt-fr-cased-small\")\n",
        "generator(\n",
        "    \"# Dans ce cours, nous vous enseignerons comment\",\n",
        "    max_length=30,  #parameters\n",
        "    num_return_sequences=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kU4aFbAPZ5a"
      },
      "source": [
        "### Remplacement des mots masqu√©s"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'argument `top_k` permet de contr√¥ler le nombre de possibilit√©s que vous souhaitez afficher. Notez que dans ce cas, le mod√®le **remplace le mot sp√©cial `<mask>`**, qui est souvent appel√© un **mot masqu√©**. D'autres mod√®les permettant de remplacer les mots manquants peuvent avoir des mots masqu√©s diff√©rents, donc il est toujours bon de v√©rifier le mot masqu√© appropri√© lorsque vous comparez d'autres mod√®les. Une fa√ßon de le v√©rifier est de regarder le mot masqu√© utilis√© dans l'outil de test de la page du mod√®le."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6UV_20PfMho_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 508/508 [00:00<00:00, 48.5kB/s]\n",
            "Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 445M/445M [00:06<00:00, 69.5MB/s] \n",
            "Downloading (‚Ä¶)tencepiece.bpe.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 811k/811k [00:01<00:00, 638kB/s]\n",
            "Downloading (‚Ä¶)/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.40M/1.40M [00:00<00:00, 1.48MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.107318215072155,\n",
              "  'token': 15328,\n",
              "  'token_str': 'automobiles',\n",
              "  'sequence': 'Ce cours vous apprendra tout sur les mod√®les automobiles.'},\n",
              " {'score': 0.07520466297864914,\n",
              "  'token': 4007,\n",
              "  'token_str': '√©lectriques',\n",
              "  'sequence': 'Ce cours vous apprendra tout sur les mod√®les √©lectriques.'}]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unmasker = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
        "unmasker(\" Ce cours vous apprendra tout sur les mod√®les <mask>.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvNljOY6Pao6"
      },
      "source": [
        "### Reconnaissance d'entit√©s nomm√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k7zh7FYMhpD"
      },
      "outputs": [],
      "source": [
        "ner = pipeline(\"ner\", model=\"Jean-Baptiste/camembert-ner\", grouped_entities=True)\n",
        "ner(\"Je m'appelle Sylvain et je travaille √† Hugging Face √† Brooklyn.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XZdeSFqnMhpF"
      },
      "source": [
        "Nous pouvons voir que le mod√®le a correctement identifi√© Sylvain comme une personne (PER), Hugging Face comme une organisation (ORG) et Brooklyn comme un lieu (LOC).\n",
        "\n",
        "Il est possible d'utiliser l'option `grouped_entities=True` lors de la cr√©ation du pipeline pour **regrouper les parties du texte qui correspondent √† la m√™me entit√© : ici le mod√®le √† correctement regroup√© `Hugging` et `Face` comme une seule organisation**, m√™me si le nom comporte plusieurs mots. En effet, comme nous allons voir dans le prochain chapitre, la pr√©traitement du texte s√©pare parfois certains mots en plus petites parties. Par exemple, `Sylvain` est s√©par√© en quatre morceaux : `S`, `##yl`, `##va`, et `##in`. Dans l'√©tape de post-traitement, le pipeline a r√©ussi √† regrouper ces morceaux."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEIu6FbnQUTb"
      },
      "source": [
        "### R√©ponse √† des questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTX2bUsgMhpG"
      },
      "outputs": [],
      "source": [
        "question_answerer = pipeline(\"question-answering\", model=\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
        "question_answerer(\n",
        "    question=\"O√π est-ce que je travaille ?\",\n",
        "    context=\"Je m'appelle Sylvain et je travaille √† Hugging Face √† Brooklyn.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZQEgjAZQW7-"
      },
      "source": [
        "###  R√©sum√©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m_mQN_jMhpJ"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"moussaKam/barthez-orangesum-abstract\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    L'Am√©rique a chang√© de fa√ßon spectaculaire au cours des derni√®res ann√©es. Non seulement le nombre de \n",
        "    dipl√¥m√©s dans les disciplines traditionnelles de l'ing√©nierie telles que le g√©nie m√©canique, civil, \n",
        "    l'√©lectricit√©, la chimie et l'a√©ronautique a diminu√©, mais dans la plupart \n",
        "    des grandes universit√©s am√©ricaines, les programmes d'√©tudes d'ing√©nierie se concentrent d√©sormais sur \n",
        "    et encouragent largement l'√©tude des sciences de l'ing√©nieur. Par cons√©quent, il y a \n",
        "    de moins en moins d'offres dans les sujets d'ing√©nierie traitant de l'infrastructure, \n",
        "    l'environnement et les questions connexes, et une plus grande concentration sur les sujets de haute \n",
        "    technologie, qui soutiennent en grande partie des d√©veloppements scientifiques de plus en plus \n",
        "    complexes. Si cette derni√®re est importante, elle ne doit pas se faire au d√©triment\n",
        "    de l'ing√©nierie plus traditionnelle.\n",
        "\n",
        "    Les √©conomies en d√©veloppement rapide telles que la Chine et l'Inde, ainsi que d'autres \n",
        "    pays industrialis√©s d'Europe et d'Asie, continuent d'encourager et de promouvoir\n",
        "    l'enseignement de l'ing√©nierie. La Chine et l'Inde, respectivement, dipl√¥ment \n",
        "    six et huit fois plus d'ing√©nieurs traditionnels que les √âtats-Unis. \n",
        "    Les autres pays industriels maintiennent au minimum leur production, tandis que l'Am√©rique \n",
        "    souffre d'une baisse de plus en plus importante du nombre de dipl√¥m√©s en ing√©nierie\n",
        "    et un manque d'ing√©nieurs bien form√©s.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Ombhj_QrDc"
      },
      "source": [
        "###  Traduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTIsplaDMhpM"
      },
      "outputs": [],
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "translator(\"This course is produced by Hugging Face.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "8e1b92f1bd4f66ce89e48c280fa4bce86ed3836ccea4e4551198da49830f5661"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
