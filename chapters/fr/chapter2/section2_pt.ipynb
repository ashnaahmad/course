{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLyzrZ_9jZBo"
      },
      "source": [
        "# Derri√®re le pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Y9E6BLjZBp"
      },
      "source": [
        "Installez la biblioth√®que ü§ó *Transformers* pour ex√©cuter ce *notebook*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WHPIDze6jZBq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (1.24.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (3.9.0)\n",
            "Requirement already satisfied: requests in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (2.28.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (3.20.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (1.26.14)\n",
            "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the '/Users/ashnaahmad/Documents/GitHub/course/.env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages (from torch) (4.5.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the '/Users/ashnaahmad/Documents/GitHub/course/.env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \"transformers[sentencepiece]\"\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B36oTdtRjZBr"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"tblard/tf-allocine\")\n",
        "classifier(\n",
        "    [\"J'ai attendu un cours d'HuggingFace toute ma vie.\",\n",
        "     \"Je d√©teste tellement √ßa !\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme nous l'avons vu dans le [chapitre 1](/course/fr/chapter1), ce pipeline regroupe trois √©tapes : le pr√©traitement, le passage des entr√©es dans le mod√®le et le post-traitement.\n",
        "\n",
        "Pour ce faire, nous utilisons un *tokenizer*, qui sera responsable de :\n",
        "- **diviser l'entr√©e en mots, sous-mots, ou symboles** (comme la ponctuation) qui sont appel√©s *tokens*,\n",
        "- **associer chaque *token* √† un nombre entier**,\n",
        "- **ajouter des entr√©es suppl√©mentaires** qui peuvent √™tre utiles au mod√®le.\n",
        "\n",
        "Tout ce pr√©traitement doit √™tre effectu√© exactement de la m√™me mani√®re que celui appliqu√© lors du pr√©-entra√Ænement du mod√®le. **Nous devons donc d'abord t√©l√©charger ces informations depuis le [*Hub*](https://huggingface.co/models). Pour ce faire, nous utilisons la classe `AutoTokenizer` et sa m√©thode `from_pretrained()`**. En utilisant le nom du *checkpoint* de notre mod√®le, elle va automatiquement **r√©cup√©rer les donn√©es associ√©es au *tokenizer* du mod√®le** et les mettre en cache (afin qu'elles ne soient t√©l√©charg√©es que la premi√®re fois que vous ex√©cutez le code ci-dessous).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tPrTkOvLjZBs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ashnaahmad/Documents/GitHub/course/.env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.00/2.00 [00:00<00:00, 245B/s]\n",
            "Downloading (‚Ä¶)tencepiece.bpe.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 811k/811k [00:00<00:00, 1.04MB/s]\n",
            "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [00:00<00:00, 41.6kB/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"tblard/tf-allocine\" # pre-traitement, effectu√© du m√™me mani√®re que le pre-entra√Ænement du mod√®le\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# **Checkpoints** : ce sont les **poids qui seront charg√©s** dans une architecture donn√©e."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une fois que nous avons le *tokenizer* nous pouvons lui passer directement nos phrases et obtenir un dictionnaire pr√™t √† √™tre donn√© √† notre mod√®le ! La seule chose qui reste √† faire est de convertir en tenseurs la liste des identifiants d'entr√©e.\n",
        "\n",
        "Vous pouvez utiliser ü§ó *Transformers* sans avoir √† vous soucier du *framework* utilis√© comme *backend*. Il peut s'agir de PyTorch, de TensorFlow ou de Flax pour certains mod√®les. Cependant, **les *transformers* n'acceptent que les *tenseurs* en entr√©e.** Si c'est la premi√®re fois que vous entendez parler de tenseurs, vous pouvez les consid√©rer comme des tableaux NumPy. Un tableau NumPy peut √™tre un scalaire (0D), un vecteur (1D), une matrice (2D), ou avoir davantage de dimensions. Les tenseurs des autres *frameworks* d'apprentissage machine se comportent de mani√®re similaire et sont g√©n√©ralement aussi simples √† instancier que les tableaux NumPy.\n",
        "\n",
        "Pour sp√©cifier le type de tenseurs que nous voulons r√©cup√©rer (PyTorch, TensorFlow, ou simplement NumPy), nous utilisons l'argument `return_tensors` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CrpoLdjKjZBt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    5,   121,    11,    73,  4903,    23,   307,    18,    11, 10312,\n",
            "         20841,   449,  5702,   194,   155,   157,     9,     6],\n",
            "        [    5,   100,  8645,  1217,   136,    83,     6,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"J'ai attendu un cours d'HuggingFace toute ma vie.\",\n",
        "    \"Je d√©teste tellement √ßa !\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La sortie elle-m√™me est un dictionnaire contenant deux cl√©s : `input_ids` et `attention_mask`. `input_ids` contient deux lignes d'entiers (une pour chaque phrase) qui sont les identifiants uniques des *tokens* dans chaque phrase. Nous expliquerons ce qu'est l'`attention_mask` plus tard dans ce chapitre. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous pouvons **t√©l√©charger notre mod√®le pr√©-entra√Æn√©** de la m√™me mani√®re que nous l'avons fait avec notre *tokenizer*. ü§ó *Transformers* fournit une classe `AutoModel` qui poss√®de √©galement une m√©thode `from_pretrained()` :\n",
        "\n",
        "Dans cet extrait de code, **nous avons t√©l√©charg√© le m√™me *checkpoint*** que nous avons utilis√© dans notre pipeline auparavant (il devrait en fait avoir d√©j√† √©t√© mis en cache) et instanci√© un mod√®le avec lui.\n",
        "\n",
        "# **Checkpoints** : ce sont les **poids qui seront charg√©s** dans une architecture donn√©e.\n",
        "\n",
        "Cette architecture ne contient que **le module de *transformer* de base** : **√©tant donn√© certaines entr√©es, il produit ce que nous appellerons des *√©tats cach√©s***, √©galement connus sous le nom de *caract√©ristiques*. \n",
        "Pour chaque entr√©e du mod√®le, nous r√©cup√©rons un vecteur en grande dimension repr√©sentant la **compr√©hension contextuelle de cette entr√©e par le *transformer***.\n",
        "\n",
        "Si cela ne fait pas sens, ne vous inqui√©tez pas. Nous expliquons tout plus tard.\n",
        "\n",
        "Bien que ces √©tats cach√©s puissent √™tre utiles en eux-m√™mes, ils sont g√©n√©ralement **les entr√©es d'une autre partie du mod√®le, connue sous le nom de *t√™te***. Dans le [chapitre 1](/course/fr/chapter1), les diff√©rentes t√¢ches auraient pu √™tre **r√©alis√©es avec la m√™me architecture mais en ayant chacune d'elles une t√™te diff√©rente.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(repeat cell for a reminder of what's going on in the cell below)\n",
        "\n",
        "Tout ce pr√©traitement doit √™tre effectu√© exactement de la m√™me mani√®re que celui appliqu√© lors du pr√©-entra√Ænement du mod√®le. **Nous devons donc d'abord t√©l√©charger ces informations depuis le [*Hub*](https://huggingface.co/models). Pour ce faire, nous utilisons la classe `AutoTokenizer` et sa m√©thode `from_pretrained()`**. En utilisant le nom du *checkpoint* de notre mod√®le, elle va automatiquement **r√©cup√©rer les donn√©es associ√©es au *tokenizer* du mod√®le** et les mettre en cache (afin qu'elles ne soient t√©l√©charg√©es que la premi√®re fois que vous ex√©cutez le code ci-dessous).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0DxXSDsLjZBv"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "philschmid/pt-tblard-tf-allocine does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModel\n\u001b[1;32m      3\u001b[0m checkpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mphilschmid/pt-tblard-tf-allocine\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(checkpoint, from_tf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/Documents/GitHub/course/.env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:464\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    463\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/course/.env/lib/python3.8/site-packages/transformers/modeling_utils.py:2253\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2248\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2249\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mWEIGHTS_NAME\u001b[39m}\u001b[39;00m\u001b[39m but there is a file for Flax weights. Use `from_flax=True` to load\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2250\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m this model from those weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2251\u001b[0m             )\n\u001b[1;32m   2252\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2253\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2254\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mWEIGHTS_NAME\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2255\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[39m}\u001b[39;00m\u001b[39m or \u001b[39m\u001b[39m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2256\u001b[0m             )\n\u001b[1;32m   2257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   2258\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[39m# to the original exception.\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[0;31mOSError\u001b[0m: philschmid/pt-tblard-tf-allocine does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack."
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"philschmid/pt-tblard-tf-allocine\"\n",
        "model = AutoModel.from_pretrained(checkpoint, from_tf=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans cet extrait de code (ci-dessus), nous avons t√©l√©charg√© le m√™me *checkpoint* que nous avons utilis√© dans notre pipeline auparavant (il devrait en fait avoir d√©j√† √©t√© mis en cache) et instanci√© un mod√®le avec lui.\n",
        "\n",
        "Cette architecture ne contient que le module de *transformer* de base : √©tant donn√© certaines entr√©es, il produit ce que nous appellerons des *√©tats cach√©s*, √©galement connus sous le nom de *caract√©ristiques*. \n",
        "Pour chaque entr√©e du mod√®le, nous r√©cup√©rons un vecteur en grande dimension repr√©sentant la **compr√©hension contextuelle de cette entr√©e par le *transformer***. (ahhhhh)\n",
        "\n",
        "Si cela ne fait pas sens, ne vous inqui√©tez pas. Nous expliquons tout plus tard.\n",
        "\n",
        "**les √©tats cach√©s (qui correspondent √† un certain ensemble de valeurs d'entr√©e) - sont repr√©sent√©s par un vecteur et repr√©sentent la comprehension contextuelle du mod√®le. cette comprehension se compose normalement d'entr√©es qui se trouvent ailleurs dans le mod√®le. cet ensemble d'entr√©es constitue la t√™te du mod√®le. chaque t√™te peut correspondre √† une r√©alisation d'une t√¢che diff√©rente avec l'architecture.**\n",
        "\n",
        "Bien que ces √©tats cach√©s puissent √™tre utiles en eux-m√™mes, ils sont g√©n√©ralement les entr√©es d'une autre partie du mod√®le, connue sous le nom de *t√™te*. Dans le [chapitre 1](/course/fr/chapter1), les diff√©rentes t√¢ches auraient pu √™tre r√©alis√©es avec la m√™me architecture mais en ayant chacune d'elles une t√™te diff√©rente."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Le vecteur produit en sortie par le *transformer* est g√©n√©ralement de grande dimension**. Il a g√©n√©ralement trois dimensions :\n",
        "\n",
        "- **la taille du lot** : le **nombre de s√©quences trait√©es √† la fois** (2 dans notre exemple),\n",
        "- **la longueur de la s√©quence** : la longueur de la repr√©sentation num√©rique de la s√©quence (16 dans notre exemple),\n",
        "- **la taille cach√©e** : la **dimension du vecteur** de **chaque entr√©e** du mod√®le.\n",
        "\n",
        "On dit qu'il est de ¬´ grande dimension ¬ª en raison de la derni√®re valeur. La taille cach√©e peut √™tre tr√®s grande (g√©n√©ralement 768 pour les petits mod√®les et pour les grands mod√®les cela peut atteindre 3072 voire plus).\n",
        "\n",
        "**Nous pouvons le constater** si nous alimentons notre mod√®le avec les entr√©es que nous avons pr√©trait√©es :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q1nptXxjZBw"
      },
      "outputs": [],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)\n",
        "# prints torch.size() object\n",
        "# you can access elements of the output via index or attribute/key as in a dictionary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model input -> transformer (embeddings, layers) -> hidden states (contextual understanding) -> head -> model output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les t√™tes des mod√®les **prennent en entr√©e le vecteur de grande dimension des √©tats cach√©s** et le projettent sur une autre dimension. Elles sont g√©n√©ralement compos√©es d'une ou de quelques **couches lin√©aires** :\n",
        "\n",
        "\n",
        "**La sortie du *transformer* est envoy√©e directement √† la t√™te du mod√®le pour √™tre trait√©e.**\n",
        "Dans ce diagramme, le mod√®le est repr√©sent√© par sa couche d‚Äôench√¢ssement et les couches suivantes. **La couche d‚Äôench√¢ssement convertit chaque identifiant d'entr√©e dans l'entr√©e tokenis√©e en un vecteur qui repr√©sente le *token* associ√©**. Les couches suivantes manipulent ces vecteurs en utilisant le m√©canisme d'attention pour produire la repr√©sentation finale des phrases.\n",
        "\n",
        "les t√™tes sont adapt√©s aux t√¢ches - comme le premier param√®tre de la fonction pipeline\n",
        "\n",
        "tant pis les t√™tes sont import√©es\n",
        "\n",
        "Il existe de nombreuses architectures diff√©rentes disponibles dans la biblioth√®que ü§ó *Transformers*, chacune √©tant con√ßue autour de la prise en charge d'une t√¢che sp√©cifique. En voici une liste non exhaustive :\n",
        "- `*Model` (r√©cup√©rer les √©tats cach√©s)\n",
        "- `*ForCausalLM`\n",
        "- `*ForMaskedLM`\n",
        "- `*ForMultipleChoice`\n",
        "- `*ForQuestionAnswering`\n",
        "- `*ForSequenceClassification`\n",
        "- `*ForTokenClassification`\n",
        "- et autres ü§ó"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour notre exemple, nous avons besoin d'un mod√®le avec une t√™te de classification de s√©quence (pour pouvoir classer les phrases comme positives ou n√©gatives). Donc, nous n'utilisons pas r√©ellement la classe `AutoModel` mais plut√¥t `AutoModelForSequenceClassification` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4G0umTzjZBw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"tblard/tf-allocine\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, from_tf=True)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maintenant, si nous examinons la forme de nos entr√©es, la dimensionnalit√© est beaucoup plus faible. La t√™te du mod√®le prend en entr√©e les vecteurs de grande dimension que nous avons vus pr√©c√©demment et elle produit des vecteurs contenant deux valeurs (une par √©tiquette) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H1v3vPCjZBx"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme nous n'avons que deux phrases et deux √©tiquettes, le r√©sultat que nous obtenons est de forme 2 x 2\n",
        "\n",
        "## Post-traitement de la sortie\n",
        "\n",
        "Les valeurs que nous obtenons en sortie de notre mod√®le n'ont pas n√©cessairement de sens en elles-m√™mes. Jetons-y un coup d‚Äô≈ìil  :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYV9Vi44jZBy"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notre mod√®le a pr√©dit `[-1.5607, 1.6123]` pour la premi√®re phrase et `[ 4.1692, -3.3464]` pour la seconde. **Ce ne sont pas des probabilit√©s mais des *logits*, les scores bruts, non normalis√©s, produits par la derni√®re couche du mod√®le**. Pour √™tre convertis en probabilit√©s, **ils doivent passer par une couche [SoftMax](https://fr.wikipedia.org/wiki/Fonction_softmax)** (tous les mod√®les de la biblioth√®que ü§ó *Transformers* sortent les logits car **la fonction de perte de l'entra√Ænement fusionne g√©n√©ralement la derni√®re fonction d'activation, comme la SoftMax, avec la fonction de perte r√©elle, comme l'entropie crois√©e**) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMhKHB14jZBy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maintenant nous pouvons voir que le mod√®le a pr√©dit `[0.0402, 0.9598]` pour la premi√®re phrase et `[0.9995, 0.0005]` pour la seconde. Ce sont des scores de probabilit√© reconnaissables.\n",
        "\n",
        "Pour obtenir les √©tiquettes correspondant √† chaque position, **nous pouvons inspecter l'attribut `id2label` de la configuration du mod√®le** (plus de d√©tails dans la section suivante)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maJACxAxjZBz"
      },
      "outputs": [],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Nous pouvons maintenant conclure que le mod√®le a pr√©dit ce qui suit :\n",
        " \n",
        "- premi√®re phrase : NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
        "- deuxi√®me phrase : NEGATIVE: 0.9995, POSITIVE: 0.0005\n",
        "\n",
        "Nous avons reproduit avec succ√®s les trois √©tapes du pipeline : pr√©traitement avec les *tokenizers*, passage des entr√©es dans le mod√®le et post-traitement ! Prenons maintenant le temps de nous plonger plus profond√©ment dans chacune de ces √©tapes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Derri√®re le pipeline (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "8e1b92f1bd4f66ce89e48c280fa4bce86ed3836ccea4e4551198da49830f5661"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
