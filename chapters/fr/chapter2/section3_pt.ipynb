{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sYvRYFUvpEm"
      },
      "source": [
        "# Models (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lPibRBovpEo"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOav4sU5vpEo"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans cette section, nous allons examiner de plus pr√®s la cr√©ation et l'utilisation d'un mod√®le. Nous utiliserons la classe `AutoModel` qui est pratique lorsque vous voulez instancier n'importe quel mod√®le √† partir d'un checkpoint.\n",
        "\n",
        "La classe `AutoModel` et toutes les classes apparent√©es sont en fait de simples *wrappers* sur la grande vari√©t√© de mod√®les disponibles dans la biblioth√®que. C'est une enveloppe intelligente car elle peut automatiquement deviner l'architecture appropri√©e pour votre *checkpoint* et ensuite instancier un mod√®le avec cette architecture.\n",
        "\n",
        "Cependant, si vous connaissez le type de mod√®le que vous voulez utiliser, vous pouvez utiliser directement la classe qui d√©finit son architecture. Voyons comment cela fonctionne avec un mod√®le BERT.\n",
        "\n",
        "La premi√®re chose que nous devons faire pour initialiser un mod√®le BERT est de charger un objet configuration :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in4ziBIXvpEo"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# Building the config\n",
        "config = BertConfig()\n",
        "\n",
        "# Building the model from the config\n",
        "model = BertModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvb2diIMvpEp",
        "outputId": "7fb17944-707a-479f-9f64-f53d33701563"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  [...]\n",
              "  \"hidden_size\": 768,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  [...]\n",
              "}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bien que vous n'ayez pas encore vu ce que font tous ces attributs, vous devriez en reconna√Ætre certains : l'attribut `hidden_size` d√©finit la taille du vecteur `hidden_states`, et `num_hidden_layers` d√©finit le nombre de couches que le *transformer* poss√®de.\n",
        "\n",
        "### Diff√©rentes m√©thodes de chargement\n",
        "\n",
        "Un mod√®le cr√©√© √† partir de la configuration par d√©faut est **initialis√© avec des valeurs al√©atoires** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOdzCo_JvpEq"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig()\n",
        "model = BertModel(config)\n",
        "\n",
        "# Model is randomly initialized!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le mod√®le peut √™tre utilis√© tel quel mais il produira du charabia. En effet, il doit d'abord √™tre entra√Æn√©. Nous pourrions entra√Æner le mod√®le √† partir de z√©ro sur la t√¢che qui nous int√©resse mais comme vous l'avez vu dans le [chapitre 1](/course/fr/chapter1) cela n√©cessiterait beaucoup de temps et de donn√©es. De plus cela aurait un impact environnemental non n√©gligeable. Pour √©viter les efforts inutiles et redondants, il est imp√©ratif de pouvoir partager et r√©utiliser les mod√®les qui ont d√©j√† √©t√© entra√Æn√©s.\n",
        "\n",
        "**Charger un *transformer* qui a d√©j√† √©t√© entra√Æn√© est simple : nous pouvons le faire en utilisant la m√©thode `from_pretrained()`:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzhSZNPVvpEq"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme vu pr√©c√©demment, nous pouvons remplacer `BertModel` par la classe √©quivalente `AutoModel`. **A partir de maintenant nous ferons cela car cela produit un code agnostique *checkpoint*, c‚Äôest-√†-dire que si votre code fonctionne pour un *checkpoint* donn√©, il devrait fonctionner sans probl√®me avec un autre.** Cela s'applique m√™me si l'architecture est diff√©rente du moment que le *checkpoint* a √©t√© entra√Æn√© pour une t√¢che similaire (par exemple, une t√¢che d'analyse de sentiments)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans l'exemple de code ci-dessus, nous n'avons pas utilis√© `BertConfig` et **avons √† la place charg√© un mod√®le pr√©-entra√Æn√© via l'identifiant `bert-base-cased`**. Il s'agit d'un *checkpoint* qui a √©t√© entra√Æn√© par les auteurs de BERT eux-m√™mes. Vous pouvez trouver davantage de d√©tails √† son sujet dans la [fiche du mod√®le](https://huggingface.co/bert-base-cased).\n",
        "\n",
        "**Ce mod√®le est maintenant initialis√© avec tous les poids du *checkpoint***. **Il peut √™tre utilis√© directement pour l'inf√©rence sur les t√¢ches sur lesquelles il a √©t√© entra√Æn√©**. Il peut √©galement √™tre *finetun√©* sur une nouvelle t√¢che. En entra√Ænant avec des poids pr√©-entra√Æn√©s plut√¥t qu'√† partir de z√©ro, nous pouvons rapidement obtenir de bons r√©sultats.\n",
        "\n",
        "**Les poids ont √©t√© t√©l√©charg√©s et mis en cache (afin que les futurs appels √† la m√©thode `from_pretrained()` ne les ret√©l√©chargent pas)** dans le dossier cache, qui est par d√©faut *~/.cache/huggingface/transformers*. Vous pouvez personnaliser votre dossier de cache en d√©finissant la variable d'environnement `HF_HOME`.\n",
        "\n",
        "L'identifiant utilis√© pour charger le mod√®le peut √™tre l'identifiant de n'importe quel mod√®le sur le *Model Hub* du moment qu'il est compatible avec l'architecture BERT. La liste compl√®te des *checkpoints* de BERT disponibles peut √™tre trouv√©e [ici](https://huggingface.co/models?filter=bert)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sauvegarder un mod√®le est aussi facile que d'en charger un. Nous utilisons la m√©thode `save_pretrained()`, qui est analogue √† la m√©thode `from_pretrained()` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C--s1GnJvpEq"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si vous jetez un coup d'≈ìil au fichier ***config.json*, vous reconna√Ætrez les attributs n√©cessaires pour construire l'architecture du mod√®le**. Ce fichier contient √©galement certaines m√©tadonn√©es, comme l'origine du *checkpoint* et la version de la biblioth√®que ü§ó *Transformers* que vous utilisiez lors du dernier enregistrement du point *checkpoint*."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le fichier *pytorch_model.bin* est connu comme le *dictionnaire d'√©tat*. **Il contient tous les poids de votre mod√®le.** Les deux fichiers vont de pair : la configuration est n√©cessaire pour conna√Ætre l'architecture de votre mod√®le, tandis que les poids du mod√®le sont les param√®tres de votre mod√®le."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maintenant que vous savez comment charger et sauvegarder un mod√®le, essayons de l'utiliser pour faire quelques pr√©dictions. Les *transformers* ne peuvent traiter que des nombres. **Des nombres que le *tokenizer* g√©n√®re.** Mais avant de parler des *tokenizers*, explorons les entr√©es que le mod√®le accepte.\n",
        "\n",
        "Les *tokenizers* se chargent de passer les entr√©es vers les tenseurs du *framework* appropri√©. Pour vous aider √† comprendre ce qui se passe, jetons un coup d'≈ìil rapide √† ce qui doit √™tre fait avant d'envoyer les entr√©es au mod√®le.\n",
        "\n",
        "Disons que nous avons les s√©quences suivantes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvepPtw7vpEq"
      },
      "outputs": [],
      "source": [
        "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le *tokenizer* les convertit en indices de vocabulaire qui sont g√©n√©ralement appel√©s *input IDs*. Chaque s√©quence est maintenant une liste de nombres ! La sortie r√©sultante est :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN6X7ph5vpEr"
      },
      "outputs": [],
      "source": [
        "encoded_sequences = [\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il s'agit d'une liste de s√©quences encod√©es : une liste de listes. Les tenseurs n'acceptent que des formes rectangulaires (pensez aux matrices). **Ce ¬´ tableau ¬ª est d√©j√† de forme rectangulaire, donc le convertir en tenseur est facile** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB631AYAvpEr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model_inputs = torch.tensor(encoded_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkKQfRw1vpEr"
      },
      "outputs": [],
      "source": [
        "output = model(model_inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bien que le mod√®le accepte un grand nombre d'arguments diff√©rents, seuls les identifiants d'entr√©e sont n√©cessaires. Nous expliquerons plus tard ce que font les autres arguments et quand ils sont n√©cessaires. Avant cela, regardons de plus pr√®s les *tokenizers*, cet outil qui construit les entr√©es qu'un *transformer* peut comprendre."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Models (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "8e1b92f1bd4f66ce89e48c280fa4bce86ed3836ccea4e4551198da49830f5661"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
