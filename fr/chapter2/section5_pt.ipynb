{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ5FM47CINAk"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yX0MPzqINAn"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maYK36ptINAo"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYK4CqVHINAo",
        "outputId": "cea0aec7-b27f-4b42-b2d3-1cf8ca7abbad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d5H1NPBINAq",
        "outputId": "5921394a-a64f-496d-a63f-9f2fb194bc27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
              "          2607,  2026,  2878,  2166,  1012,   102]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pourquoi cela a √©chou√© ? Nous avons suivi les √©tapes du pipeline de la section 2.\n",
        "\n",
        "Le probl√®me est que nous avons envoy√© une seule s√©quence au mod√®le, alors que les mod√®les de l‚ÄôAPI ü§ó *Transformers* attendent plusieurs phrases par d√©faut. Ici, nous avons essay√© de faire ce que le *tokenizer* fait en coulisses lorsque nous l'avons appliqu√© √† une `s√©quence`. Cependant si vous regardez de pr√®s, vous verrez qu'il n'a pas seulement converti la liste des identifiants d'entr√©e en un tenseur mais aussi ajout√© une dimension par-dessus (hence dimension = 1 and indexerror)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Essayons √† nouveau en ajoutant une nouvelle dimension :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-_zMWJlINAq",
        "outputId": "1a3873bf-1993-4574-a3ac-d6004d33e09d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\n",
              "Logits: [[-2.7276,  2.8789]]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le ¬´ *batching* ¬ª est l'acte d'envoyer plusieurs phrases √† travers le mod√®le, toutes en m√™me temps. Si vous n'avez qu'une seule phrase, vous pouvez simplement construire un batch avec une seule s√©quence : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h08-LsDVINAq"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utiliser des *batchs* permet au mod√®le de fonctionner lorsque vous lui donnez plusieurs s√©quences. Utiliser plusieurs s√©quences est aussi simple que de construire un batch avec une seule s√©quence. Il y a cependant un deuxi√®me probl√®me. Lorsque vous essayez de regrouper deux phrases (ou plus), elles peuvent √™tre de longueurs diff√©rentes. Si vous avez d√©j√† travaill√© avec des tenseurs, vous savez qu'ils doivent √™tre de forme rectangulaire. Vous ne pourrez donc pas convertir directement la liste des identifiants d'entr√©e en un tenseur. Pour contourner ce probl√®me, nous avons l'habitude de *rembourrer*/*remplir* (le *padding* en anglais) les entr√©es.\n",
        "\n",
        "Le *padding* permet de s'assurer que toutes nos phrases ont la m√™me longueur en ajoutant un mot sp√©cial appel√© *padding token* aux phrases ayant moins de valeurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq4XuuqJINAq"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'identifiant du jeton de *padding* peut √™tre trouv√© dans `tokenizer.pad_token_id`. Utilisons-le et envoyons nos deux phrases √† travers le mod√®le premi√®rement individuellement puis en √©tant mises dans un m√™me batch :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW-vXATvINAq",
        "outputId": "e0c2ac19-e99f-414f-da30-e263a8598467"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
              "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
              "tensor([[ 1.5694, -1.3895],\n",
              "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc5SotmYINAr",
        "outputId": "d18b817c-0871-4bff-a0c7-c4e5610569b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.5694, -1.3895],\n",
              "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfu5-x3bINAs"
      },
      "outputs": [],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "8e1b92f1bd4f66ce89e48c280fa4bce86ed3836ccea4e4551198da49830f5661"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
